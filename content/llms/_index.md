---
title: "LLMs"
description: "AI & LLM security — prompt injection, jailbreaking, adversarial ML, RAG attacks."
---

Security research on large language models — prompt injection, jailbreaking, model extraction, indirect injection via RAG pipelines, and AI-augmented offensive techniques.
